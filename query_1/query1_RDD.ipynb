{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64868d01-bde4-4f3d-955d-caef06e86d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1765289937462_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0002_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>5</td><td>application_1765289937462_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0006_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>6</td><td>application_1765289937462_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0007_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>9</td><td>application_1765289937462_0010</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:8088/cluster/app/application_1765289937462_0010\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:8188/applicationhistory/logs/ip-192-168-1-48.eu-central-1.compute.internal:8041/container_1765289937462_0010_01_000001/container_1765289937462_0010_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202a1bc8-2036-41e6-a988-dbb0a081360c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a418ad22b4fdda3e33af31bb9d7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def parse_csv(line):\n",
    "    return next(csv.reader([line]))\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "rdd1 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\")\\\n",
    "            .map(parse_csv)\n",
    "\n",
    "rdd2 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\")\\\n",
    "            .map(parse_csv)\n",
    "\n",
    "crime_rdd_raw = rdd1.union(rdd2)\n",
    "\n",
    "# header = crime_rdd_raw.first()\n",
    "# crime_rdd_raw = crime_rdd_raw.filter(lambda line: line != header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f59dc306-76d1-43be-b1d8-e8a1462d9958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc5ecf845764b9b9650b9a988e70ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adults 121660\n",
      "Young Adults 33758\n",
      "Children 10904\n",
      "Elderly 6011\n",
      "Execution time: 25.0686616897583 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def parse_age(s):\n",
    "    try:\n",
    "        age = int(s)\n",
    "        return age if age > 0 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def age_bucket(age):\n",
    "    if age is None:\n",
    "        return None\n",
    "    if age < 18:\n",
    "        return \"Children\"\n",
    "    elif age <= 24:\n",
    "        return \"Young Adults\"\n",
    "    elif age <= 64:\n",
    "        return \"Adults\"\n",
    "    else:\n",
    "        return \"Elderly\"\n",
    "\n",
    "start = time.time()\n",
    "agg_buckets = (\n",
    "    crime_rdd_raw\n",
    "    .filter(lambda r: len(r) > 11 and r[9] and \"aggravated assault\" in r[9].lower())\n",
    "    .map(lambda r: (age_bucket(parse_age(r[11])), 1))\n",
    "    .filter(lambda x: x[0] is not None)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "for group, cnt in agg_buckets.collect():\n",
    "    print(group, cnt)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution time:\", end - start, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9756eb70-7784-4f24-8e6b-e4490b960588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffdc9f6ba064f63b36242d106ef7e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4.0 failed 4 times, most recent failure: Lost task 1.3 in stage 4.0 (TID 62) (ip-192-168-1-91.eu-central-1.compute.internal executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 3983, in combineLocally\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/util.py\", line 97, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 6, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '\"10\"'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2501)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2522)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2541)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2566)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1050)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:152)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:113)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1049)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 3983, in combineLocally\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/util.py\", line 97, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 6, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '\"10\"'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 1572, in sortBy\n",
      "    self.keyBy(keyfunc)  # type: ignore[type-var]\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 1509, in sortByKey\n",
      "    rddSize = self.count()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 2316, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 2291, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 2044, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 1833, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4.0 failed 4 times, most recent failure: Lost task 1.3 in stage 4.0 (TID 62) (ip-192-168-1-91.eu-central-1.compute.internal executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 3983, in combineLocally\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/util.py\", line 97, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 6, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '\"10\"'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1047)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2501)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2522)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2541)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2566)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1050)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:152)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:113)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1049)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000001/pyspark.zip/pyspark/rdd.py\", line 3983, in combineLocally\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_0011/container_1765289937462_0011_01_000014/pyspark.zip/pyspark/util.py\", line 97, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 6, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '\"10\"'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:152)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_buckets = (\n",
    "    crime_rdd_raw\n",
    "    # keep only aggravated assaults\n",
    "    .filter(lambda r: len(r) > 11 and r[9] is not None and \"aggravated assault\" in r[9].lower())\n",
    "    # map to (bucket, 1)\n",
    "    .map(lambda r: (age_bucket(parse_age(r[11])), 1))\n",
    "    # drop rows with no valid bucket\n",
    "    .filter(lambda x: x[0] is not None)\n",
    "    # count per bucket\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    # sort by count descending\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "result = agg_buckets.collect()\n",
    "\n",
    "for group, cnt in result:\n",
    "    print(group, cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f0ccf-1809-4303-82ba-c9e0acefcb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
