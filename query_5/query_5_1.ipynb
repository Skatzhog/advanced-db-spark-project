{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f0f4ef-14a8-4c97-b7c1-eb5b7f5572ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>981</td><td>application_1765289937462_0974</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0974/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0974_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>982</td><td>application_1765289937462_0975</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0975/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-131.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0975_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>985</td><td>application_1765289937462_0978</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0978/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-131.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0978_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>986</td><td>application_1765289937462_0979</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0979/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-103.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0979_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>999</td><td>application_1765289937462_0992</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0992/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0992_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1000</td><td>application_1765289937462_0993</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0993/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-213.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0993_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1480b286-cc25-47a0-aaf8-88934be4a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1001</td><td>application_1765289937462_0994</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0994/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0994_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c3ad2037524df6ab3d194f5661e39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eb49eb8d034f0aa524ff2ae07db38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BG20: string (nullable = true)\n",
      " |-- BG20FIP_CURRENT: string (nullable = true)\n",
      " |-- BGFIP20: string (nullable = true)\n",
      " |-- CB20: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- CITYCOMM: string (nullable = true)\n",
      " |-- CITYCOMM_CURRENT: string (nullable = true)\n",
      " |-- CITY_CURRENT: string (nullable = true)\n",
      " |-- COMM: string (nullable = true)\n",
      " |-- COMM_CURRENT: string (nullable = true)\n",
      " |-- COUNTY: string (nullable = true)\n",
      " |-- CT20: string (nullable = true)\n",
      " |-- CTCB20: string (nullable = true)\n",
      " |-- FEAT_TYPE: string (nullable = true)\n",
      " |-- FIP20: string (nullable = true)\n",
      " |-- FIP_CURRENT: string (nullable = true)\n",
      " |-- HD22: long (nullable = true)\n",
      " |-- HD_NAME: string (nullable = true)\n",
      " |-- HOUSING20: long (nullable = true)\n",
      " |-- OBJECTID: long (nullable = true)\n",
      " |-- POP20: long (nullable = true)\n",
      " |-- SPA22: long (nullable = true)\n",
      " |-- SPA_NAME: string (nullable = true)\n",
      " |-- SUP21: string (nullable = true)\n",
      " |-- SUP_LABEL: string (nullable = true)\n",
      " |-- ShapeSTArea: double (nullable = true)\n",
      " |-- ShapeSTLength: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZCTA20: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import udf, year, avg, count, concat, lit, round, rank, col, regexp_replace, substring, corr\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"query 4 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", header = True, inferSchema = True)\n",
    "# crime\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \n",
    "# census\n",
    "\n",
    "median_household_income= \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "# median_df = spark.read.csv(median_household_income, header = True, inferSchema = True)\n",
    "median_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"delimiter\", \";\") \n",
    "    .csv(median_household_income)\n",
    ")\n",
    "#median_household_income\n",
    "\n",
    "# Print schema\n",
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552476d4-1ac3-4500-8440-1a26c932faa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec28e5ba4514c01b3729bc7bcbc6559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "#Filter data only from LA, do summations for Housing and Population\n",
    "group_flattened = (\n",
    "    flattened_df\n",
    "    .select(\"COMM\", \"POP20\", \"ZCTA20\", \"HOUSING20\" , \"geometry\")\n",
    "    .filter(\n",
    "        (col(\"ZCTA20\") > 0) &\n",
    "        (col(\"HOUSING20\") > 0) &\n",
    "        (col(\"POP20\") > 0) &\n",
    "        (col(\"COMM\") != \"\")\n",
    "    )\n",
    "    .groupBy(\"COMM\", \"ZCTA20\")\n",
    "    .agg(\n",
    "        _sum(\"POP20\").alias(\"Total_POP\"),\n",
    "        _sum(\"HOUSING20\").alias(\"Total_Housing\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47097e4d-ef73-4323-b5e1-b16306e65f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca8f61ecbc240ec833a7eecb34c5ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[COMM#124], functions=[sum(ZIP_Total_Income#350), sum(Total_POP#299L), st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "   +- Exchange hashpartitioning(COMM#124, 1000), ENSURE_REQUIREMENTS, [plan_id=143]\n",
      "      +- ObjectHashAggregate(keys=[COMM#124], functions=[partial_sum(ZIP_Total_Income#350), partial_sum(Total_POP#299L), partial_st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "         +- Project [COMM#124, Total_POP#299L, geometry#306, (cast(regexp_replace(Estimated Median Income#284, [^0-9], , 1) as double) * cast(Total_Housing#301L as double)) AS ZIP_Total_Income#350]\n",
      "            +- BroadcastHashJoin [cast(ZCTA20#144 as int)], [Zip Code#282], Inner, BuildRight, false\n",
      "               :- ObjectHashAggregate(keys=[COMM#124, ZCTA20#144], functions=[sum(POP20#136L), sum(HOUSING20#134L), st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "               :  +- Exchange hashpartitioning(COMM#124, ZCTA20#144, 1000), ENSURE_REQUIREMENTS, [plan_id=135]\n",
      "               :     +- ObjectHashAggregate(keys=[COMM#124, ZCTA20#144], functions=[partial_sum(POP20#136L), partial_sum(HOUSING20#134L), partial_st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "               :        +- Project [features#107.properties.COMM AS COMM#124, features#107.properties.POP20 AS POP20#136L, features#107.properties.ZCTA20 AS ZCTA20#144, features#107.properties.HOUSING20 AS HOUSING20#134L, features#107.geometry AS geometry#110]\n",
      "               :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "               :              +- Generate explode(features#99), false, [features#107]\n",
      "               :                 +- Filter ((size(features#99, true) > 0) AND isnotnull(features#99))\n",
      "               :                    +- FileScan geojson [features#99] Batched: false, DataFilters: [(size(features#99, true) > 0), isnotnull(features#99)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=138]\n",
      "                  +- Filter isnotnull(Zip Code#282)\n",
      "                     +- FileScan csv [Zip Code#282,Estimated Median Income#284] Batched: false, DataFilters: [isnotnull(Zip Code#282)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>"
     ]
    }
   ],
   "source": [
    "# Join Census population/housing data (group_flattened) with median income data (median_df)\n",
    "# Join key: ZCTA20 (from Census) == Zip Code (from income dataset)\n",
    "joined = (\n",
    "    group_flattened\n",
    "    .join(median_df, group_flattened[\"ZCTA20\"] == median_df[\"Zip Code\"]) #Broadcast Hash Join\n",
    "    .withColumn(\n",
    "        \"Estimated Median Income\", \n",
    "        regexp_replace(col(\"Estimated Median Income\"), \"[^0-9]\", \"\") # Remove non-digits: \"$68,000\" or \"68,000\". -> 68000\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ZIP_Total_Income\", \n",
    "        (col(\"Estimated Median Income\") * col(\"Total_Housing\"))\n",
    "    )\n",
    "    .groupBy(\"COMM\") # Aggregate at the community (\"COMM\") level\n",
    "    .agg(\n",
    "        _sum(\"Total_POP\").alias(\"Total_COMM_Pop\"),\n",
    "        _sum(\"ZIP_Total_Income\").alias(\"COMM_Total_Income\"),\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\") # merges ZIP shapes into COMM shape\n",
    "    )\n",
    "    # Compute GDP per capita per community\n",
    "    .withColumn(\n",
    "        \"GDP_Per_Capita\",\n",
    "        (col(\"COMM_Total_Income\")/col(\"Total_COMM_Pop\"))\n",
    "    )\n",
    "    .select(\"COMM\", \"GDP_Per_Capita\", \"geometry\") \n",
    ")\n",
    "\n",
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a730acb1-9d50-4428-9c12-8accc48e63ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1cc6bc961c42108d3f1e3e2240baff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#keep crime data only from 2020 and 2021. Also create a geometry column according to longtitude and latitude columns.\n",
    "crime_data_20_21_df = (\n",
    "    crime_df\n",
    "    .filter(\n",
    "        (substring(col(\"DATE OCC\"), 1, 4) == \"2020\") |\n",
    "        (substring(col(\"DATE OCC\"), 1, 4) == \"2021\") \n",
    "    )\n",
    "    .select(\"DATE OCC\", \"LAT\", \"LON\")\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "    .drop(\"LAT\", \"LON\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d091f3-4475-402c-9513-d324a640dd1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f66c09bfea944d59de80e925355de0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#124], functions=[count(1)], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#124, 1000), ENSURE_REQUIREMENTS, [plan_id=297]\n",
      "      +- HashAggregate(keys=[COMM#124], functions=[partial_count(1)], schema specialized)\n",
      "         +- Project [COMM#124]\n",
      "            +- RangeJoin geometry#377: geometry, geom#437: geometry, CONTAINS\n",
      "               :- Filter isnotnull(geometry#377)\n",
      "               :  +- ObjectHashAggregate(keys=[COMM#124], functions=[st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "               :     +- Exchange hashpartitioning(COMM#124, 1000), ENSURE_REQUIREMENTS, [plan_id=290]\n",
      "               :        +- ObjectHashAggregate(keys=[COMM#124], functions=[partial_st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "               :           +- Project [COMM#124, geometry#306]\n",
      "               :              +- BroadcastHashJoin [cast(ZCTA20#144 as int)], [Zip Code#282], Inner, BuildRight, false\n",
      "               :                 :- ObjectHashAggregate(keys=[COMM#124, ZCTA20#144], functions=[st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "               :                 :  +- Exchange hashpartitioning(COMM#124, ZCTA20#144, 1000), ENSURE_REQUIREMENTS, [plan_id=282]\n",
      "               :                 :     +- ObjectHashAggregate(keys=[COMM#124, ZCTA20#144], functions=[partial_st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "               :                 :        +- Project [features#107.properties.COMM AS COMM#124, features#107.properties.ZCTA20 AS ZCTA20#144, features#107.geometry AS geometry#110]\n",
      "               :                 :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "               :                 :              +- Generate explode(features#99), false, [features#107]\n",
      "               :                 :                 +- Filter ((size(features#99, true) > 0) AND isnotnull(features#99))\n",
      "               :                 :                    +- FileScan geojson [features#99] Batched: false, DataFilters: [(size(features#99, true) > 0), isnotnull(features#99)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "               :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=285]\n",
      "               :                    +- Filter isnotnull(Zip Code#282)\n",
      "               :                       +- FileScan csv [Zip Code#282] Batched: false, DataFilters: [isnotnull(Zip Code#282)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int>\n",
      "               +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#437]\n",
      "                  +- Filter (((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     +- FileScan csv [DATE OCC#44,LAT#68,LON#69] Batched: false, DataFilters: [((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>"
     ]
    }
   ],
   "source": [
    "final_crimes_df = (\n",
    "    joined\n",
    "    .join(crime_data_20_21_df, ST_Within(crime_data_20_21_df.geom, joined.geometry), \"inner\") # Range Join\n",
    "    .groupBy(\"COMM\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"#\")\n",
    "    )\n",
    ")\n",
    "\n",
    "final_crimes_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b2e0e9-6cdc-4952-bb06-6395473240b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a4854c131e4b05ab60df633744fd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#124, (cast(##460L as double) / cast(Total_Pop#299L as double)) AS Crimes_Per_Capita#535]\n",
      "   +- BroadcastHashJoin [COMM#124], [COMM#483], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=523]\n",
      "      :  +- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[sum(POP20#136L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#124, ZCTA20#144, 1000), ENSURE_REQUIREMENTS, [plan_id=503]\n",
      "      :        +- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[partial_sum(POP20#136L)], schema specialized)\n",
      "      :           +- Project [features#107.properties.COMM AS COMM#124, features#107.properties.POP20 AS POP20#136L, features#107.properties.ZCTA20 AS ZCTA20#144]\n",
      "      :              +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "      :                 +- Generate explode(features#99), false, [features#107]\n",
      "      :                    +- Filter ((size(features#99, true) > 0) AND isnotnull(features#99))\n",
      "      :                       +- FileScan geojson [features#99] Batched: false, DataFilters: [(size(features#99, true) > 0), isnotnull(features#99)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      +- HashAggregate(keys=[COMM#483], functions=[count(1)], schema specialized)\n",
      "         +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=520]\n",
      "            +- HashAggregate(keys=[COMM#483], functions=[partial_count(1)], schema specialized)\n",
      "               +- Project [COMM#483]\n",
      "                  +- RangeJoin geometry#377: geometry, geom#437: geometry, CONTAINS\n",
      "                     :- Filter isnotnull(geometry#377)\n",
      "                     :  +- ObjectHashAggregate(keys=[COMM#483], functions=[st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                     :     +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=513]\n",
      "                     :        +- ObjectHashAggregate(keys=[COMM#483], functions=[partial_st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                     :           +- Project [COMM#483, geometry#306]\n",
      "                     :              +- BroadcastHashJoin [cast(ZCTA20#503 as int)], [Zip Code#282], Inner, BuildRight, false\n",
      "                     :                 :- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                     :                 :  +- Exchange hashpartitioning(COMM#483, ZCTA20#503, 1000), ENSURE_REQUIREMENTS, [plan_id=505]\n",
      "                     :                 :     +- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[partial_st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                     :                 :        +- Project [features#107.properties.COMM AS COMM#483, features#107.properties.ZCTA20 AS ZCTA20#503, features#107.geometry AS geometry#110]\n",
      "                     :                 :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "                     :                 :              +- Generate explode(features#472), false, [features#107]\n",
      "                     :                 :                 +- Filter ((size(features#472, true) > 0) AND isnotnull(features#472))\n",
      "                     :                 :                    +- FileScan geojson [features#472] Batched: false, DataFilters: [(size(features#472, true) > 0), isnotnull(features#472)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "                     :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=508]\n",
      "                     :                    +- Filter isnotnull(Zip Code#282)\n",
      "                     :                       +- FileScan csv [Zip Code#282] Batched: false, DataFilters: [isnotnull(Zip Code#282)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int>\n",
      "                     +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#437]\n",
      "                        +- Filter (((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                           +- FileScan csv [DATE OCC#44,LAT#68,LON#69] Batched: false, DataFilters: [((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>"
     ]
    }
   ],
   "source": [
    "#join Cencus with crimes per COMM. Join key is Zip Code. Result is crimes per capita in every area.\n",
    "crime_joined = (\n",
    "    group_flattened\n",
    "    .join(final_crimes_df, on=\"COMM\", how=\"inner\") # Broadcast Hash Join\n",
    "    .withColumn(\n",
    "        \"Crimes_Per_Capita\",\n",
    "        (col(\"#\")/col(\"Total_Pop\"))\n",
    "    )\n",
    "    .select(\"COMM\", \"Crimes_Per_Capita\")\n",
    ")\n",
    "\n",
    "top_10 = joined.orderBy(col(\"GDP_Per_Capita\"), ascending=False).limit(10)\n",
    "\n",
    "bottom_10 = joined.orderBy(col(\"GDP_Per_Capita\"), ascending=True).limit(10)\n",
    "\n",
    "crime_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84616ea-1f07-441c-bb7f-7028a904bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce4dc6eabae42a3b43ce1eccf8c1500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data Correlation = 0.05699006354516788\n",
      "Full data correlation execution time: 23.830943822860718 seconds\n",
      "Top 10 areas Correlation = 0.2930302995361155\n",
      "Top 10 areas execution time: 16.838924646377563 seconds\n",
      "Bottom 10 areas Correlation = 0.17968480714580115\n",
      "Bottom 10 areas execution time: 15.748841047286987 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "import time\n",
    "\n",
    "#Corelation while using all areas\n",
    "start1 = time.time()\n",
    "corr_joined = (\n",
    "    joined\n",
    "    .join(crime_joined, on=\"COMM\", how=\"inner\") # Sort Merge Join\n",
    "    .select(\"COMM\", \"GDP_Per_Capita\", \"Crimes_Per_Capita\")\n",
    ")\n",
    "\n",
    "full_corr = corr_joined.select(\n",
    "    corr(\"GDP_Per_Capita\", \"Crimes_Per_Capita\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(\"Full data Correlation =\", full_corr)\n",
    "end1 = time.time()\n",
    "print(\"Full data correlation execution time:\", end1 - start1, \"seconds\")\n",
    "\n",
    "#Corelation while using top10 income areas\n",
    "start2 = time.time()\n",
    "corr_top10 = (\n",
    "    top_10\n",
    "    .join(crime_joined, on=\"COMM\", how=\"inner\") # Broadcast Hash Join\n",
    "    .select(\"COMM\", \"GDP_Per_Capita\", \"Crimes_Per_Capita\") \n",
    ")\n",
    "\n",
    "full_corr_top10 = corr_top10.select(\n",
    "    corr(\"GDP_Per_Capita\", \"Crimes_Per_Capita\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(\"Top 10 areas Correlation =\", full_corr_top10)\n",
    "end2 = time.time()\n",
    "print(\"Top 10 areas execution time:\", end2 - start2, \"seconds\")\n",
    "\n",
    "#Corelation while using the bottom 10 areas\n",
    "start3 = time.time()\n",
    "corr_bottom10 = (\n",
    "    bottom_10\n",
    "    .join(crime_joined, on=\"COMM\", how=\"inner\") # Broadcast Hash Join\n",
    "    .select(\"COMM\", \"GDP_Per_Capita\", \"Crimes_Per_Capita\") \n",
    ")\n",
    "\n",
    "full_corr_bot10 = corr_bottom10.select(\n",
    "    corr(\"GDP_Per_Capita\", \"Crimes_Per_Capita\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(\"Bottom 10 areas Correlation =\", full_corr_bot10)\n",
    "end3 = time.time()\n",
    "print(\"Bottom 10 areas execution time:\", end3 - start3, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7ad5f3-7622-49b1-9890-0369dceb0a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39f0ca9cc2f4d3bb37fe17302d9a8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#124, GDP_Per_Capita#394, Crimes_Per_Capita#535]\n",
      "   +- SortMergeJoin [COMM#124], [COMM#569], Inner\n",
      "      :- Sort [COMM#124 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#124], functions=[sum(ZIP_Total_Income#350), sum(Total_POP#299L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#124, 1000), ENSURE_REQUIREMENTS, [plan_id=6425]\n",
      "      :        +- HashAggregate(keys=[COMM#124], functions=[partial_sum(ZIP_Total_Income#350), partial_sum(Total_POP#299L)], schema specialized)\n",
      "      :           +- Project [COMM#124, Total_POP#299L, (cast(regexp_replace(Estimated Median Income#284, [^0-9], , 1) as double) * cast(Total_Housing#301L as double)) AS ZIP_Total_Income#350]\n",
      "      :              +- BroadcastHashJoin [cast(ZCTA20#144 as int)], [Zip Code#282], Inner, BuildRight, false\n",
      "      :                 :- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[sum(POP20#136L), sum(HOUSING20#134L)], schema specialized)\n",
      "      :                 :  +- Exchange hashpartitioning(COMM#124, ZCTA20#144, 1000), ENSURE_REQUIREMENTS, [plan_id=6417]\n",
      "      :                 :     +- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[partial_sum(POP20#136L), partial_sum(HOUSING20#134L)], schema specialized)\n",
      "      :                 :        +- Project [features#107.properties.COMM AS COMM#124, features#107.properties.POP20 AS POP20#136L, features#107.properties.ZCTA20 AS ZCTA20#144, features#107.properties.HOUSING20 AS HOUSING20#134L]\n",
      "      :                 :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "      :                 :              +- Generate explode(features#99), false, [features#107]\n",
      "      :                 :                 +- Filter ((size(features#99, true) > 0) AND isnotnull(features#99))\n",
      "      :                 :                    +- FileScan geojson [features#99] Batched: false, DataFilters: [(size(features#99, true) > 0), isnotnull(features#99)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6420]\n",
      "      :                    +- Filter isnotnull(Zip Code#282)\n",
      "      :                       +- FileScan csv [Zip Code#282,Estimated Median Income#284] Batched: false, DataFilters: [isnotnull(Zip Code#282)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- Sort [COMM#569 ASC NULLS FIRST], false, 0\n",
      "         +- Filter bloomfilter#1834 of [bf1834 COMM#124 estimatedNumRows=255300] filtering [COMM#569]\n",
      "            :  +- GenerateBloomFilter bf1834, 255300, 0, false, false, true, true, [id=#6559]\n",
      "            :     +- OutputAdapter [COMM#124, ZCTA20#144, sum#410L, sum#412L]\n",
      "            :        +- AdaptiveSparkPlan isFinalPlan=false\n",
      "            :           +- Exchange hashpartitioning(COMM#124, ZCTA20#144, 1000), ENSURE_REQUIREMENTS, [plan_id=6556]\n",
      "            :              +- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[partial_sum(POP20#136L), partial_sum(HOUSING20#134L)], schema specialized)\n",
      "            :                 +- Project [features#107.properties.COMM AS COMM#124, features#107.properties.POP20 AS POP20#136L, features#107.properties.ZCTA20 AS ZCTA20#144, features#107.properties.HOUSING20 AS HOUSING20#134L]\n",
      "            :                    +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "            :                       +- Generate explode(features#99), false, [features#107]\n",
      "            :                          +- Filter ((size(features#99, true) > 0) AND isnotnull(features#99))\n",
      "            :                             +- FileScan geojson [features#99] Batched: false, DataFilters: [(size(features#99, true) > 0), isnotnull(features#99)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "            +- Project [COMM#569, (cast(##460L as double) / cast(Total_Pop#299L as double)) AS Crimes_Per_Capita#535]\n",
      "               +- BroadcastHashJoin [COMM#569], [COMM#483], Inner, BuildLeft, false\n",
      "                  :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6447]\n",
      "                  :  +- HashAggregate(keys=[COMM#569, ZCTA20#589], functions=[sum(POP20#581L)], schema specialized)\n",
      "                  :     +- Exchange hashpartitioning(COMM#569, ZCTA20#589, 1000), ENSURE_REQUIREMENTS, [plan_id=6427]\n",
      "                  :        +- HashAggregate(keys=[COMM#569, ZCTA20#589], functions=[partial_sum(POP20#581L)], schema specialized)\n",
      "                  :           +- Project [features#107.properties.COMM AS COMM#569, features#107.properties.POP20 AS POP20#581L, features#107.properties.ZCTA20 AS ZCTA20#589]\n",
      "                  :              +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "                  :                 +- Generate explode(features#555), false, [features#107]\n",
      "                  :                    +- Filter ((size(features#555, true) > 0) AND isnotnull(features#555))\n",
      "                  :                       +- FileScan geojson [features#555] Batched: false, DataFilters: [(size(features#555, true) > 0), isnotnull(features#555)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "                  +- HashAggregate(keys=[COMM#483], functions=[count(1)], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=6444]\n",
      "                        +- HashAggregate(keys=[COMM#483], functions=[partial_count(1)], schema specialized)\n",
      "                           +- Project [COMM#483]\n",
      "                              +- RangeJoin geometry#377: geometry, geom#437: geometry, CONTAINS\n",
      "                                 :- Filter isnotnull(geometry#377)\n",
      "                                 :  +- ObjectHashAggregate(keys=[COMM#483], functions=[st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                                 :     +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=6437]\n",
      "                                 :        +- ObjectHashAggregate(keys=[COMM#483], functions=[partial_st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                                 :           +- Project [COMM#483, geometry#306]\n",
      "                                 :              +- BroadcastHashJoin [cast(ZCTA20#503 as int)], [Zip Code#558], Inner, BuildRight, false\n",
      "                                 :                 :- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                                 :                 :  +- Exchange hashpartitioning(COMM#483, ZCTA20#503, 1000), ENSURE_REQUIREMENTS, [plan_id=6429]\n",
      "                                 :                 :     +- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[partial_st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                                 :                 :        +- Project [features#107.properties.COMM AS COMM#483, features#107.properties.ZCTA20 AS ZCTA20#503, features#107.geometry AS geometry#110]\n",
      "                                 :                 :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "                                 :                 :              +- Generate explode(features#472), false, [features#107]\n",
      "                                 :                 :                 +- Filter ((size(features#472, true) > 0) AND isnotnull(features#472))\n",
      "                                 :                 :                    +- FileScan geojson [features#472] Batched: false, DataFilters: [(size(features#472, true) > 0), isnotnull(features#472)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "                                 :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6432]\n",
      "                                 :                    +- Filter isnotnull(Zip Code#558)\n",
      "                                 :                       +- FileScan csv [Zip Code#558] Batched: false, DataFilters: [isnotnull(Zip Code#558)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int>\n",
      "                                 +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#437]\n",
      "                                    +- Filter (((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                       +- FileScan csv [DATE OCC#44,LAT#68,LON#69] Batched: false, DataFilters: [((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>"
     ]
    }
   ],
   "source": [
    "corr_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d39fe5e9-d609-46f9-8557-08b7d4bd8018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf9151c7ce6491b85b09f5e3b69cfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#124, GDP_Per_Capita#394, Crimes_Per_Capita#535]\n",
      "   +- BroadcastHashJoin [COMM#124], [COMM#1051], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6886]\n",
      "      :  +- TakeOrderedAndProject(limit=10, orderBy=[GDP_Per_Capita#394 DESC NULLS LAST], output=[COMM#124,GDP_Per_Capita#394])\n",
      "      :     +- HashAggregate(keys=[COMM#124], functions=[sum(ZIP_Total_Income#350), sum(Total_POP#299L)], schema specialized)\n",
      "      :        +- Exchange hashpartitioning(COMM#124, 1000), ENSURE_REQUIREMENTS, [plan_id=6859]\n",
      "      :           +- HashAggregate(keys=[COMM#124], functions=[partial_sum(ZIP_Total_Income#350), partial_sum(Total_POP#299L)], schema specialized)\n",
      "      :              +- Project [COMM#124, Total_POP#299L, (cast(regexp_replace(Estimated Median Income#284, [^0-9], , 1) as double) * cast(Total_Housing#301L as double)) AS ZIP_Total_Income#350]\n",
      "      :                 +- BroadcastHashJoin [cast(ZCTA20#144 as int)], [Zip Code#282], Inner, BuildRight, false\n",
      "      :                    :- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[sum(POP20#136L), sum(HOUSING20#134L)], schema specialized)\n",
      "      :                    :  +- Exchange hashpartitioning(COMM#124, ZCTA20#144, 1000), ENSURE_REQUIREMENTS, [plan_id=6851]\n",
      "      :                    :     +- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[partial_sum(POP20#136L), partial_sum(HOUSING20#134L)], schema specialized)\n",
      "      :                    :        +- Project [features#107.properties.COMM AS COMM#124, features#107.properties.POP20 AS POP20#136L, features#107.properties.ZCTA20 AS ZCTA20#144, features#107.properties.HOUSING20 AS HOUSING20#134L]\n",
      "      :                    :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "      :                    :              +- Generate explode(features#99), false, [features#107]\n",
      "      :                    :                 +- Filter ((size(features#99, true) > 0) AND isnotnull(features#99))\n",
      "      :                    :                    +- FileScan geojson [features#99] Batched: false, DataFilters: [(size(features#99, true) > 0), isnotnull(features#99)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6854]\n",
      "      :                       +- Filter isnotnull(Zip Code#282)\n",
      "      :                          +- FileScan csv [Zip Code#282,Estimated Median Income#284] Batched: false, DataFilters: [isnotnull(Zip Code#282)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- Project [COMM#1051, (cast(##460L as double) / cast(Total_Pop#299L as double)) AS Crimes_Per_Capita#535]\n",
      "         +- BroadcastHashJoin [COMM#1051], [COMM#483], Inner, BuildLeft, false\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6882]\n",
      "            :  +- HashAggregate(keys=[COMM#1051, ZCTA20#1071], functions=[sum(POP20#1063L)], schema specialized)\n",
      "            :     +- Exchange hashpartitioning(COMM#1051, ZCTA20#1071, 1000), ENSURE_REQUIREMENTS, [plan_id=6862]\n",
      "            :        +- HashAggregate(keys=[COMM#1051, ZCTA20#1071], functions=[partial_sum(POP20#1063L)], schema specialized)\n",
      "            :           +- Project [features#107.properties.COMM AS COMM#1051, features#107.properties.POP20 AS POP20#1063L, features#107.properties.ZCTA20 AS ZCTA20#1071]\n",
      "            :              +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "            :                 +- Generate explode(features#1037), false, [features#107]\n",
      "            :                    +- Filter ((size(features#1037, true) > 0) AND isnotnull(features#1037))\n",
      "            :                       +- FileScan geojson [features#1037] Batched: false, DataFilters: [(size(features#1037, true) > 0), isnotnull(features#1037)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "            +- HashAggregate(keys=[COMM#483], functions=[count(1)], schema specialized)\n",
      "               +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=6879]\n",
      "                  +- HashAggregate(keys=[COMM#483], functions=[partial_count(1)], schema specialized)\n",
      "                     +- Project [COMM#483]\n",
      "                        +- RangeJoin geometry#377: geometry, geom#437: geometry, CONTAINS\n",
      "                           :- Filter isnotnull(geometry#377)\n",
      "                           :  +- ObjectHashAggregate(keys=[COMM#483], functions=[st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :     +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=6872]\n",
      "                           :        +- ObjectHashAggregate(keys=[COMM#483], functions=[partial_st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :           +- Project [COMM#483, geometry#306]\n",
      "                           :              +- BroadcastHashJoin [cast(ZCTA20#503 as int)], [Zip Code#1040], Inner, BuildRight, false\n",
      "                           :                 :- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :                 :  +- Exchange hashpartitioning(COMM#483, ZCTA20#503, 1000), ENSURE_REQUIREMENTS, [plan_id=6864]\n",
      "                           :                 :     +- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[partial_st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :                 :        +- Project [features#107.properties.COMM AS COMM#483, features#107.properties.ZCTA20 AS ZCTA20#503, features#107.geometry AS geometry#110]\n",
      "                           :                 :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "                           :                 :              +- Generate explode(features#472), false, [features#107]\n",
      "                           :                 :                 +- Filter ((size(features#472, true) > 0) AND isnotnull(features#472))\n",
      "                           :                 :                    +- FileScan geojson [features#472] Batched: false, DataFilters: [(size(features#472, true) > 0), isnotnull(features#472)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "                           :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6867]\n",
      "                           :                    +- Filter isnotnull(Zip Code#1040)\n",
      "                           :                       +- FileScan csv [Zip Code#1040] Batched: false, DataFilters: [isnotnull(Zip Code#1040)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int>\n",
      "                           +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#437]\n",
      "                              +- Filter (((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                 +- FileScan csv [DATE OCC#44,LAT#68,LON#69] Batched: false, DataFilters: [((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#124, GDP_Per_Capita#394, Crimes_Per_Capita#535]\n",
      "   +- BroadcastHashJoin [COMM#124], [COMM#1453], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=7272]\n",
      "      :  +- TakeOrderedAndProject(limit=10, orderBy=[GDP_Per_Capita#394 ASC NULLS FIRST], output=[COMM#124,GDP_Per_Capita#394])\n",
      "      :     +- HashAggregate(keys=[COMM#124], functions=[sum(ZIP_Total_Income#350), sum(Total_POP#299L)], schema specialized)\n",
      "      :        +- Exchange hashpartitioning(COMM#124, 1000), ENSURE_REQUIREMENTS, [plan_id=7245]\n",
      "      :           +- HashAggregate(keys=[COMM#124], functions=[partial_sum(ZIP_Total_Income#350), partial_sum(Total_POP#299L)], schema specialized)\n",
      "      :              +- Project [COMM#124, Total_POP#299L, (cast(regexp_replace(Estimated Median Income#284, [^0-9], , 1) as double) * cast(Total_Housing#301L as double)) AS ZIP_Total_Income#350]\n",
      "      :                 +- BroadcastHashJoin [cast(ZCTA20#144 as int)], [Zip Code#282], Inner, BuildRight, false\n",
      "      :                    :- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[sum(POP20#136L), sum(HOUSING20#134L)], schema specialized)\n",
      "      :                    :  +- Exchange hashpartitioning(COMM#124, ZCTA20#144, 1000), ENSURE_REQUIREMENTS, [plan_id=7237]\n",
      "      :                    :     +- HashAggregate(keys=[COMM#124, ZCTA20#144], functions=[partial_sum(POP20#136L), partial_sum(HOUSING20#134L)], schema specialized)\n",
      "      :                    :        +- Project [features#107.properties.COMM AS COMM#124, features#107.properties.POP20 AS POP20#136L, features#107.properties.ZCTA20 AS ZCTA20#144, features#107.properties.HOUSING20 AS HOUSING20#134L]\n",
      "      :                    :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "      :                    :              +- Generate explode(features#99), false, [features#107]\n",
      "      :                    :                 +- Filter ((size(features#99, true) > 0) AND isnotnull(features#99))\n",
      "      :                    :                    +- FileScan geojson [features#99] Batched: false, DataFilters: [(size(features#99, true) > 0), isnotnull(features#99)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=7240]\n",
      "      :                       +- Filter isnotnull(Zip Code#282)\n",
      "      :                          +- FileScan csv [Zip Code#282,Estimated Median Income#284] Batched: false, DataFilters: [isnotnull(Zip Code#282)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- Project [COMM#1453, (cast(##460L as double) / cast(Total_Pop#299L as double)) AS Crimes_Per_Capita#535]\n",
      "         +- BroadcastHashJoin [COMM#1453], [COMM#483], Inner, BuildLeft, false\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=7268]\n",
      "            :  +- HashAggregate(keys=[COMM#1453, ZCTA20#1473], functions=[sum(POP20#1465L)], schema specialized)\n",
      "            :     +- Exchange hashpartitioning(COMM#1453, ZCTA20#1473, 1000), ENSURE_REQUIREMENTS, [plan_id=7248]\n",
      "            :        +- HashAggregate(keys=[COMM#1453, ZCTA20#1473], functions=[partial_sum(POP20#1465L)], schema specialized)\n",
      "            :           +- Project [features#107.properties.COMM AS COMM#1453, features#107.properties.POP20 AS POP20#1465L, features#107.properties.ZCTA20 AS ZCTA20#1473]\n",
      "            :              +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "            :                 +- Generate explode(features#1439), false, [features#107]\n",
      "            :                    +- Filter ((size(features#1439, true) > 0) AND isnotnull(features#1439))\n",
      "            :                       +- FileScan geojson [features#1439] Batched: false, DataFilters: [(size(features#1439, true) > 0), isnotnull(features#1439)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "            +- HashAggregate(keys=[COMM#483], functions=[count(1)], schema specialized)\n",
      "               +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=7265]\n",
      "                  +- HashAggregate(keys=[COMM#483], functions=[partial_count(1)], schema specialized)\n",
      "                     +- Project [COMM#483]\n",
      "                        +- RangeJoin geometry#377: geometry, geom#437: geometry, CONTAINS\n",
      "                           :- Filter isnotnull(geometry#377)\n",
      "                           :  +- ObjectHashAggregate(keys=[COMM#483], functions=[st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :     +- Exchange hashpartitioning(COMM#483, 1000), ENSURE_REQUIREMENTS, [plan_id=7258]\n",
      "                           :        +- ObjectHashAggregate(keys=[COMM#483], functions=[partial_st_union_aggr(geometry#306, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@35f6bca6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :           +- Project [COMM#483, geometry#306]\n",
      "                           :              +- BroadcastHashJoin [cast(ZCTA20#503 as int)], [Zip Code#1442], Inner, BuildRight, false\n",
      "                           :                 :- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :                 :  +- Exchange hashpartitioning(COMM#483, ZCTA20#503, 1000), ENSURE_REQUIREMENTS, [plan_id=7250]\n",
      "                           :                 :     +- ObjectHashAggregate(keys=[COMM#483, ZCTA20#503], functions=[partial_st_union_aggr(geometry#110, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6513f4a3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                           :                 :        +- Project [features#107.properties.COMM AS COMM#483, features#107.properties.ZCTA20 AS ZCTA20#503, features#107.geometry AS geometry#110]\n",
      "                           :                 :           +- Filter ((((isnotnull(features#107.properties.ZCTA20) AND isnotnull(features#107.properties.HOUSING20)) AND isnotnull(features#107.properties.POP20)) AND isnotnull(features#107.properties.COMM)) AND ((((cast(features#107.properties.ZCTA20 as int) > 0) AND (features#107.properties.HOUSING20 > 0)) AND (features#107.properties.POP20 > 0)) AND NOT (features#107.properties.COMM = )))\n",
      "                           :                 :              +- Generate explode(features#472), false, [features#107]\n",
      "                           :                 :                 +- Filter ((size(features#472, true) > 0) AND isnotnull(features#472))\n",
      "                           :                 :                    +- FileScan geojson [features#472] Batched: false, DataFilters: [(size(features#472, true) > 0), isnotnull(features#472)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "                           :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=7253]\n",
      "                           :                    +- Filter isnotnull(Zip Code#1442)\n",
      "                           :                       +- FileScan csv [Zip Code#1442] Batched: false, DataFilters: [isnotnull(Zip Code#1442)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int>\n",
      "                           +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#437]\n",
      "                              +- Filter (((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                 +- FileScan csv [DATE OCC#44,LAT#68,LON#69] Batched: false, DataFilters: [((substring(DATE OCC#44, 1, 4) = 2020) OR (substring(DATE OCC#44, 1, 4) = 2021)), isnotnull( **o..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DATE OCC:string,LAT:double,LON:double>"
     ]
    }
   ],
   "source": [
    "corr_top10.explain()\n",
    "corr_bottom10.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133ba83f-aeb2-45dd-b319-66c065f668f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83913c849ea4d9f81ddc1456da9d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = df.checkpoint() # for shorter explains\n",
    "# df = spark.createDataFrame(df.collect(), df.schema) # for shorter explains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
